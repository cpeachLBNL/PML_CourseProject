---
title: "PML Course Project - Human Activity Recognition"
author: "Chris Peach"
date: "August 23, 2014"
output: html_document
---

##Executive Summary:
The objective of this project is to create a predictive model that determines the quality of how an excercise (bicep curl) was performed.  Specifically, 6 subjects were instructed to follow 5 methods in performing the exercise.  A training dataset was provided that contains the data generated by the 6 subjects performing the exercise and the method used ('classe' variable - A, B, C, D, E).  A predictive model was then created and applied to the test data set.  

The model developed utilizes the Random Forest method and is expected to achieve approximately <b>99.5% out-of-sample accuracy</b>, based upon 10 x Random Subsampling of the training dataset into test and cross-validation data sets.  

The classes in the test data set are predicted to be:  
<table><tr><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td><td>16</td><td>17</td><td>18</td><td>19</td><td>20</td></tr><tr><td>B</td><td>A</td><td>B</td><td>A</td><td>A</td><td>E</td><td>D</td><td>B</td><td>A</td><td>A</td><td>B</td><td>C</td><td>B</td><td>A</td><td>E</td><td>E</td><td>A</td><td>B</td><td>B</td><td>B</td></tr></table> 

Code for this project is at:  https://github.com/cpeachLBNL/PML_CourseProject  

##Model Construction:
The model was built using the following approach:

<b>1)  Specify the Question of Interest</b>  
Can you create a predictive model that determines the way the exercise was performed?
Specifically, given a new (test) dataset, the predictive model needs to be able to predict the exercise method followed (i.e. A, B, C, D or E).

<b>2)  Determine What Data to Use</b>  
The train data set provided (pml-training.csv) was be used to train the model.  The training dataset has 19622 rows x 160 columns.  This dataset has sufficient rows of data to split into training and cross-validation data sets.  The 160 columns will be reduced by a combination of intuition and experimentation.

<b>3)  Clean the Data</b>  
There are several columns that are either blank or NA in the test dataset.  Therefore, as an initial strategy, these columns will be dropped.  Further, all timestamp, user_name, and window related variables are also dropped, as they intuitively should not be used in the model.  Thus, the training and test datasets are loaded and transformed as follows:


```{r loadData, results="asis", echo=TRUE}
library (ggplot2); library(caret); library(randomForest)

setwd("~/Documents/Personal/R/Coursera/PracticalMachineLearning/PML_CourseProject")
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv") 
columns <- c("roll_belt", "pitch_belt", "yaw_belt", 
             "total_accel_belt", "gyros_belt_x", "gyros_belt_y", "gyros_belt_z", "accel_belt_x", 
             "accel_belt_y", "accel_belt_z", "magnet_belt_x", "magnet_belt_y", "magnet_belt_z", 
             "roll_arm", "pitch_arm", "yaw_arm", "total_accel_arm", "gyros_arm_x", "gyros_arm_y", 
             "gyros_arm_z", "accel_arm_x", "accel_arm_y", "accel_arm_z", "magnet_arm_x", "magnet_arm_y", 
             "magnet_arm_z", "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell", "gyros_dumbbell_x", 
             "gyros_dumbbell_y", "gyros_dumbbell_z", "accel_dumbbell_x", "accel_dumbbell_y", 
             "accel_dumbbell_z", "magnet_dumbbell_x", "magnet_dumbbell_y", "magnet_dumbbell_z", 
             "roll_forearm", "pitch_forearm", "yaw_forearm", "total_accel_forearm", "gyros_forearm_x", 
             "gyros_forearm_y", "gyros_forearm_z", "accel_forearm_x", "accel_forearm_y", "accel_forearm_z", 
             "magnet_forearm_x", "magnet_forearm_y", "magnet_forearm_z")
training <- training[, c(columns, "classe")]
training$magnet_forearm_y <- as.integer(training$magnet_forearm_y)
training$magnet_forearm_z <- as.integer(training$magnet_forearm_z)
testing <- testing[,columns]
```

<b>4)  Select Statistical Prediction Model(s):</b>  
As an intial strategy, the Random Forest method is chosen as this method is known to have high accuracy, especially for classification types of problems.  If this model is found to be inaccurate or has poor performance (i.e. long run duration) then alternative methods, such as Boosting, will be explored.

In order to calculate an out-of-sample accuracy estimate, the training data set is divided into two datasets:  train and xvalid (cross validation).  The system is then trained on the train dataset and the accuracy of predictions is compared against xvalid.  This is repeated 10 times, and the average accuracy is used as the out-of-sample accuracy estimate.


```{r createModel, echo=TRUE}
lstRF <- list()
nRuns <- 10
sumAcc <- 0
for (n in 1:nRuns){
    inTrain <- createDataPartition(training$classe, p=0.8, list=FALSE)
    train <- training[inTrain,]
    xvalid <- training[-inTrain,]
    rfObj = randomForest(classe ~ ., data = train, ntree=100)
    predX <-  predict(rfObj, xvalid[,columns])
    acc <- sum(predX == xvalid$classe) / nrow(xvalid)
    sumAcc <- sumAcc + acc
    print(sprintf ("Case:  %i, Accuracy:  %.2f%%", n, acc*100))
    lstRF[[n]] <- rfObj
}
avgAcc <- sumAcc / nRuns
sprintf("Overall Accuracy:  %.2f%%", avgAcc*100)
```

<b>5)  Predict Results</b>  
Now, apply the model to the testing data set (20 records) and predict the classes:
```{r predictTestCases, echo=TRUE}
#Use each rfObj (from lstRF) to output a prediction:
lstPred <- list()
for (n in 1:nRuns){
    pred <- predict(lstRF[[n]], testing)
    lstPred[[n]] <- pred
    #Check for consistent prediction
    if (n > 1)
        print(sprintf("Pred %i same as previous: %s", n, all(pred == lstPred[[n-1]])))
}
#Since all predictions are the same, output the first one:
lstPred[[1]]
```

